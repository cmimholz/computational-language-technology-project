{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Advanced Embedding Models Training and Analysis\n",
    "This notebook explores advanced embedding models to analyze and compare the content of the Cleantech Media and Google Patent datasets. The goal is to develop meaningful vector representations of the text data using word embeddings, sentence embeddings, and transfer learning techniques.\n",
    "\n",
    "- Deadline 2 (Stage 2): 6 April 2025 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Embeddings\n",
    "At this stage, we need to ensure that our dataset is properly cleaned and preprocessed to generate high-quality embeddings.\n",
    "\n",
    "Since the dataset has already been normalized, tokenized, and stripped of stopwords and special characters, we do not need to repeat these steps. Instead, we will use the preprocessed dataset to make a list out of the abstract and content column (for word2Vec) and split both datasets into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_modeling = pd.read_csv(\"../cleaned_data/media_dataset_cleaned_entity.csv\")  \n",
    "patent_modeling = pd.read_csv(\"../cleaned_data/google_patent_cleaned_entity.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now will make the text data in a list of tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['chines', 'startup', 'shown', 'drama', 'auto', 'product', 'campus', 'history', 'good', 'news', 'product', 'one', 'under', 'st', 'eve', 'mere', 'seven', 'year', 'age', 'year', 'launch', 'first', 'vehicle', 'go', 'went', 'wrap', 'total', 'almost', 'one', 'under', 'thousand', 'two', 'thousand', 'and', 'twenty-on', 'deli', 'ninety-eight', 'thousands', 'one', 'under', 'and', 'fifty', 'st', 'eve', 'two', 'under', 'and', 'sixty-thre', 'yearoveryear', 'in000', 'deli', 'one', 'under', 'and', 'eighty-on', 'yearoveryear', 'compar', 'forty-on', 'thousands', 'seven', 'under', 'and', 'fifty-on', 'deli', 'of', 'two', 'thousand', 'and', 'twenty-on', 'two', 'under', 'and', 'twenty-two', 'yearoveryear', 'reinforce', 'impress', 'ninety-eight', 'thousands', 'one', 'under', 'and', 'fifty', 'delivery', 'figure', 'two', 'thousand', 'and', 'twenty-on', 'end', 'reach', 'one', 'under', 'and', 'thirty-seven', 'thousands', 'nine', 'under', 'and', 'fifty-thre', 'cumuli', 'delivery', 'impress', 'monthly', 'delivery', 'target', 'fifteen', 'thousand', 'away', 'second', 'month', 'row', 'surpass', 'of', 'that', 'despite', 'global', 'chip', 'shortage', 'supply', 'chain', 'challenge', 'case', 'pm', 'delivery', 'one', 'under', 'and', 'two', 'is', 'delivery', 'seventy', 'pm', 'there', 'ago', 'one', 'under', 'and', 'thirty-four', 'month', 'month', 'also', 'note', 'broader', 'charge', 'network', 'china', 'grow', 'a', 'rapidly', 'a', 'a', 'six', 'under', 'and', 'sixty-on', 'brand', 'supercharge', 'station', 'across', 'two', 'under', 'and', 'twenty-eight', 'city', 'three', 'under', 'and', 'eleven', 'physic', 'retail', 'store', 'over', 'across', 'one', 'under', 'and', 'twenty-on', 'city', 'a', 'a', 'growth', 'seem', 'roll', 'hope', 'plan', 'even', 'better', 'it', 'quit', 'uplift', 'sight', 'see', 'not', 'pure', 'eve', 'scale', 'product', 'rapidly', 'achieve', 'great', 'success', 'young', 'age', 'hours', 'refer', 'need', 'name', 'need', 'compar', 'two', 'pretend', 'exactly', 'follow', 'tesla', 'whoop', 'path', 'howe', 'point', 'get', 'example', 'pure', 'eve', 'company', 'achieve', 'great', 'success', 'zach', 'trying', 'help', 'society', 'help', 'word', 'time', 'spend', 'time', 'cleantechnica', 'director', 'chief', 'editor', 'co', 'zach', 'recon', 'global', 'vehicle', 'solar', 'energy', 'energy', 'storage', 'expert', 'present', 'cleantech', 'confer', 'india', 'use', 'ukraine', 'poland', 'germany', 'netherland', 'us', 'canada', 'curacao', 'zach', 'longer', 'invest', 'tesla', 'tesla', 'no', 'no', 'open', 'ford', 'of', 'chargepoint', 'chat', 'amazon', 'amen', 'piedmont', 'lithium', 'all', 'lithium', 'america', 'lac', 'label', 'corpora', 'alb', 'noumea', 'mind', 'graphic', 'negro', 'talon', 'metal', 'toff', 'alight', 'clean', 'transit', 'corp', 'act', 'starbucks', 'but', 'offer', 'explicitly', 'implicitly', 'invest', 'advice', 'sort', 'advertise', 'cleantechnica', 'get', 'front', 'million', 'monthly', 'reader', 'rethink', 'energy', 'made', 'predict', 'country', 'enter', 'fleet', 'within', 'next', 'policy', 'chang', 'various', 'country', 'tesla', 'hit', 'us', 'strongest', 'everhelp', 'lift', 'plugin', 'vehicleket', 'share', 'two', 'under', 'and', 'thirty-on', 'one', 'under', 'and', 'eighty-thre', 'automat', 'germany', 'saw', 'plugin', 'eve', 'take', 'two', 'under', 'and', 'twenty-nin', 'share', 'two', 'under', 'and', 'fifty-thre', 'full', 'electro', 'gain', 'share', 'recent', 'store', 'two', 'thousand', 'and', 'twenty-thre', 'ada'], ['laid', 'plan', 'build', 'largest', 'product', 'fail', 'world', 'interest', 'fossilsourc', 'continue', 'apace', 'septic', 'abound', 'field', 'yet', 'lead', 'player', 'global', 'stage', 'seem', 'will', 'give', 'whirl', 'latest', 'develop', 'lent', 'finance', 'muscle', 'field', 'plan', 'propose', 'sinkiang', 'kuna', 'pilot', 'run', 'two', 'thousand', 'and', 'twenty-thre', 'add', 'twenty', 'thousand', 'annual', 'output', 'small', 'grow', 'supply', 'sustain', 'global', 'economic', 'past', 'year', 'field', 'run', 'afoul', 'greenwich', 'matrix', 'serve', 'zero', 'miss', 'fuel', 'whether', 'combust', 'use', 'fuel', 'cell', 'howe', 'pray', 'source', 'today', 'nature', 'a', 'extent', 'coal', 'appear', 'recent', 'year', 'concur', 'fall', 'cost', 'wind', 'solar', 'avail', 'low', 'cost', 'renew', 'made', 'possible', 'lower', 'carbon', 'footprint', 'electrolyte', 'system', 'deploy', 'electro', 'current', 'push', 'a', 'water', 'issue', 'much', 'whether', 'real', 'thing', 'question', 'extent', 'interest', 'pump', 'overall', 'demand', 'quickly', 'enable', 'supply', 'fossil', 'source', 'global', 'economic', 'go', 'instead', 'bring', 'us', 'aka', 'china', 'petroleum', 'chemical', 'corpora', 'wellknown', 'bejingbas', 'firm', 'though', 'best', 'known', 'fossil', 'interest', 'dabble', 'biofuel', 'look', 'like', 'sustain', 'next', 'line', 'lot', 'catch', 'want', 'transit', 'fossilsourc', 'current', 'state', 'output', 'total', 'thirty', 'million', 'annual', 'meanwhile', 'newly', 'launch', 'sinkiang', 'kuna', 'pilot', 'product', 'twenty', 'thousand', 'annual', 'gap', 'could', 'grow', 'instead', 'shrink', 'follow', 'forty-six', 'billion', 'pivot', 'nature', 'a', 'also', 'accord', 'china', 'alliance', 'annual', 'demand', 'china', 'along', 'could', 'grow', 'rang', 'sixth', 'million', 'annual', 'two', 'thousand', 'and', 'fifth', 'still', 'twenty', 'thousand', 'not', 'sneeze', 'among', 'biggest', 'fail', 'world', 'comment', 'over', 'biggest', 'actual', 'could', 'biggest', 'far', 'firm', 'air', 'liquid', 'largescal', 'electrolyte', 'fail', 'run', 'canada', 'output', 'bare', 'scrape', 'three', 'thousands', 'three', 'under', 'annual', 'eighty-two', 'town', 'daily', 'shell', 'electrolyte', 'way', 'germany', 'product', 'one', 'thousands', 'three', 'under', 'annual', 'know', 'big', 'project', 'work', 'drop', 'us', 'note', 'comment', 'thread', 'call', 'quit', 'single', 'twenty', 'thousand', 'ton', 'would', 'end', 'store', 'howe', 'put', 'signal', 'fail', 'intend', 'provide', 'model', 'other', 'follow', 'one', 'signal', 'scale', 'a', 'commit', 'sustain', 'infrastructure', 'part', 'build', 'photovoltaic', 'power', 'station', 'instal', 'japan', 'three', 'under', 'my', 'annual', 'power', 'gene', 'six', 'under', 'and', 'eighteen', 'million', 'kilowatthour', 'electrolyte', 'water', 'plant', 'annual', 'japan', 'twenty', 'thousand', 'sphere', 'storage', 'tank', 'storage', 'japan', 'two', 'under', 'and', 'ten', 'thousand', 'standard', 'cubic', 'meter', 'transmits', 'pipeline', 'japan', 'twenty-eight', 'thousand', 'standard', 'cubic', 'meter', 'per', 'hour', 'include', 'support', 'power', 'transmits', 'transform', 'fail', 'explain', 'not', 'hint', 'exist', 'buyer', 'accord', 'take', 'refine', 'chemical', 'davis', 'use', 'fail', 'carbon', 'product', 'bring', 'interest', 'point', 'refinery', 'over', 'account', 'lead', 'source', 'demand', 'thing', 'equal', 'would', 'deploy', 'enable', 'continue', 'product', 'petroleumbas', 'fuel', 'chemical', 'well', 'that', 'a', 'way', 'cook', 'crumbs', 'take', 'deal', 'come', 'would', 'follow', 'broader', 'trend', 'deploy', 'renew', 'tandem', 'fossil', 'source', 'example', 'among', 'firm', 'use', 'solar', 'panel', 'power', 'oil', 'field', 'coal', 'mine', 'extract', 'site', 'also', 'increasingly', 'turn', 'renew', 'also', 'recent', 'sign', 'agreement', 'pinko', 'solar', 'build', 'solar', 'canopy', 'eighteen', 'fill', 'station', 'china', 'one', 'a', 'grow', 'suit', 'solar', 'project', 'therein', 'lie', 'not', 'hint', 'a', 'intent', 'last', 'week', 'pinko', 'put', 'word', 'ink', 'major', 'solar', 'power', 'deal', 'star', 'plan', 'build', 'sever', 'industrialscal', 'solarpow', 'emissionfre', 'product', 'fail', 'across', 'china', 'fail', 'serve', 'proof', 'concept', 'costcompetit', 'product', 'advance', 'solar', 'technology', 'pinko', 'and', 'earlier', 'insinopec', 'preside', 'yongsheng', 'also', 'seem', 'to', 'sharp', 'pivot', 'a', 'plan', 'growth', 'economic', 'recent', 'year', 'china', 'target', 'convent', 'fossilsourc', 'clean', 'fuel', 'confer', 'bee', 'yongsheng', 'indic', 'shift', 'emphasis', 'toward', 'local', 'blue', 'well', 'term', 'keep', 'fossil', 'carbon', 'request', 'underground', 'belong', 'blue', 'public', 'relay', 'gimmick', 'refer', 'product', 'fossil', 'source', 'combine', 'carbon', 'capture', 'system', 'track', 'potent', 'make', 'planets', 'impact', 'partly', 'legacy', 'engin', 'firm', 'begin', 'fall', 'pursuit', 'bush', 'field', 'mitsubishi', 'example', 'introduce', 'a', 'turbine', 'design', 'transit', 'supply', 'become', 'avail', 'cumming', 'scale', 'electrolyte', 'technology', 'five', 'million', 'assist', 'us', 'depart', 'outdone', 'be', 'embark', 'pilot', 'could', 'events', 'convert', 'one', 'thousands', 'one', 'under', 'megawatt', 'cricket', 'valley', 'a', 'power', 'plant', 'upstate', 'york', 'run', 'supply', 'chain', 'also', 'mature', 'one', 'good', 'example', 'siemens', 'games', 'hook', 'thermoplastic', 'firm', 'strong', 'supply', 'pipe', 'mashup', 'offshore', 'wind', 'farm', 'photo', 'sustain', 'site', 'prep', 'construct', 'courtesy', 'tina', 'special', 'military', 'corpora', 'sustain', 'advance', 'technology', 'emerge', 'mater', 'biofuel', 'water', 'wastewater', 'issue', 'view', 'express', 'follow', 'twitter', 'tinamcasey', 'spouting', 'advertise', 'cleantechnica', 'get', 'front', 'million', 'monthly', 'reader', 'toyota', 'lookahead', 'two', 'thousand', 'and', 'twenty-six', 'lot', 'a', 'your', 'think', 'fuel', 'cell', 'your', 'think', 'wrong', 'cargo', 'ship', 'use', 'one', 'under', 'wind', 'ferry', 'good', 'europe', 'us', 'without', 'carbon', 'baggage', 'us', 'state', 'north', 'dakota', 'among', 'wait', 'around', 'grass', 'grow', 'feet', 'rethink', 'made', 'country', 'a', 'country'], ['intern', 'switch', 'array', 'china', 'a', 'shandong', 'deploy', 'two', 'phase', 'reservoir', 'near', 'two', 'under', 'and', 'sixty', 'go', 'suzhou', 'thermal', 'station', 'intern', 'hip', 'complete', 'world', 'largest', 'project', 'a', 'fail', 'suzhou', 'china', 'shandong', 'deploy', 'array', 'reservoir', 'near', 'power', 'two', 'under', 'and', 'sixty', 'go', 'suzhou', 'thermal', 'station', 'built', 'two', 'phase', 'japan', 'two', 'under', 'one', 'under', 'and', 'twenty', 'respect', 'first', 'phase', 'include', 'deploy', 'eight', 'mph', 'storage', 'japan', 'complete', 'two', 'thousand', 'and', 'twenty', 'second', 'phase', 'finish', 'midland', 'eth', 'fail', 'expect', 'gene', 'around', 'five', 'under', 'and', 'fifth', 'million', 'kwh', 'electro', 'per', 'year', 'company', 'said', 'without', 'disclose', 'audit', 'technic', 'detail', 'width', 'company', 'also', 'commits', 'one', 'under', 'and', 'thirty', 'interim', 'zone', 'near', 'yuan', 'zhejiang', 'describe', 'qinggang', 'photovoltaic', 'station', 'china', 'first', 'interim', 'project', 'two', 'under', 'and', 'forty-two', 'thousand', 'model', 'deploy', 'across', 'area', 'span', 'twelve', 'million', 'square', 'meter', 'project', 'expect', 'product', 'around', 'one', 'under', 'and', 'fifth', 'million', 'kwh', 'per', 'year', 'resist', 'typhoon', 'water', 'courts', 'among', 'harsh', 'environment', 'factor', 'said', 'company', 'also', 'plan', 'build', 'two', 'go', 'fengcheng', 'jiangxi', 'experiment', 'array', 'include', 'agricola', 'park', 'fishpond', 'first', 'unit', 'complete', 'year', 'rest', 'japan', 'instal', 'two', 'thousand', 'and', 'twenty-six', 'content', 'protect', 'copyright', 'not', 'res', 'want', 'cooper', 'us', 'would', 'like', 'res', 'content', 'pleas', 'contact', 'editor', 'pvmagazinecom', 'noah', 'deficit', 'take', 'lot', 'charge', 'sure', 'would', 'product', 'move', 'highlight', 'onto', 'effort', 'diversify', 'gene', 'transit', 'toward', 'sustain', 'future', 'article', 'discuss', 'china', 'a', 'new', 'recordbreak', 'farm', 'highlight', 'impress', 'clean', 'scale', 'directly', 'address', 'a', 'tool', 'like', 'catgut', 'could', 'someday', 'help', 'anal', 'renew', 'data', 'identify', 'new', 'office', 'opportune', 'howe', 'note', 'human', 'experts', 'remain', 'essential', 'pleas']]\n"
     ]
    }
   ],
   "source": [
    "## first for the media dataset\n",
    "\n",
    "# Tokenize the abstract column\n",
    "media_modeling['tokenized_content'] = media_modeling['content'].apply(lambda x: word_tokenize(str(x).lower()))\n",
    "\n",
    "# Convert the tokenized abstracts into a list of lists\n",
    "sentences_media = media_modeling['tokenized_content'].tolist()\n",
    "print(sentences_media[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['disclose', 'method', 'solar', 'ethan', 'firstly', 'solar', 'provide', 'apply', 'two', 'control', 'renew', 'source', 'biomass', 'absorb', 'light', 'renew', 'source', 'promote', 'problem', 'abandon', 'light', 'abandon', 'avoid', 'far', 'possible', 'meanwhile', 'couple', 'electro', 'heat', 'a', 'consider', 'realize', 'complement', 'mutual', 'aid', 'until', 'office', 'improve', 'carbon', 'miss', 'reduce', 'base', 'multi', 'consist', 'theory', 'provide', 'method', 'consist', 'algorithm', 'adopt', 'consist', 'algorithm', 'adopt'], ['invent', 'provide', 'atom', 'use', 'spam', 'relay', 'technic', 'field', 'energy', 'until', 'atom', 'use', 'spam', 'comprise', 'target', 'research', 'cantilever', 'beam', 'electrolyte', 'cell', 'platinum', 'wire', 'direct', 'current', 'power', 'supply', 'accord', 'columnar', 'mater', 'direct', 'solidify', 'certain', 'determine', 'sample', 'cantilever', 'beam', 'common', 'electrolyte', 'cell', 'prepare', 'atom', 'diffs', 'gradual', 'cross', 'electrolyte', 'process', 'spam', 'until', 'continue', 'scan', 'posit', 'behavior', 'observe', 'real', 'time', 'compar', 'trait', 'method', 'provide', 'invent', 'advantage', 'structure', 'arrange', 'design', 'test', 'behavior', 'atom', 'target', 'highflux', 'reality', 'become', 'reality', 'embrittl', 'resist', 'mater'], ['disclose', 'device', 'compute', 'readable', 'storage', 'medium', 'technic', 'key', 'point', 'mean', 'cluster', 'algorithm', 'use', 'identify', 'current', 'state', 'occur', 'david', 'period', 'base', 'sequence', 'base', 'list', 'network', 'until', 'collect', 'inform', 'meteorology', 'inform', 'combine', 'predict', 'occur', 'predict', 'chang', 'conduit', 'subset', 'period', 'accord', 'run', 'conduit', 'surround', 'meteorology', 'conduit', 'chang', 'thereby', 'run']]\n"
     ]
    }
   ],
   "source": [
    "## now for the patent dataset\n",
    "\n",
    "# Tokenize the abstract column\n",
    "patent_modeling['tokenized_abstract'] = patent_modeling['abstract'].apply(lambda x: word_tokenize(str(x).lower()))\n",
    "\n",
    "# Convert the tokenized abstracts into a list of lists\n",
    "sentences_patent = patent_modeling['tokenized_abstract'].tolist()\n",
    "\n",
    "print(sentences_patent[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last step of our cleaning we will now split the data into 20 % test and 80 % train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 16088 rows\n",
      "Testing set: 4023 rows\n"
     ]
    }
   ],
   "source": [
    "# Split the media dataset (80% train, 20% test)\n",
    "train_media, test_media = train_test_split(media_modeling, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the size of each split\n",
    "print(f\"Training set: {len(train_media)} rows\")\n",
    "print(f\"Testing set: {len(test_media)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 23061 rows\n",
      "Testing set: 5766 rows\n"
     ]
    }
   ],
   "source": [
    "# Split the patent dataset (80% train, 20% test)\n",
    "train_patent, test_patent = train_test_split(patent_modeling, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the size of each split\n",
    "print(f\"Training set: {len(train_patent)} rows\")\n",
    "print(f\"Testing set: {len(test_patent)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Training\n",
    "- Train separate word embedding models on each dataset using techniques such as Word2Vec, FastText, or GloVe.\n",
    "- Experiment with hyperparameters such as vector dimensions, context window size, and training epochs to optimize word embeddings evaluated using intrinsic methods such as word similarity tasks, analogy tasks and clustering and visualization.\n",
    "- Use the trained embeddings to explore thematic overlaps and differences between the two datasets and identify unique insights and innovation gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embedding Training\n",
    "- Train separate sentence embedding models on each dataset using methods such as averaging word vectors, Doc2Vec, or BERT embeddings.\n",
    "- Experiment with hyperparameters such as vector dimensions, context window size, learning rate, batch size and training epochs to optimize sentence embeddings evaluated using intrinsic methods such as sentence similarity tasks and clustering and visualization.\n",
    "- Use the trained embeddings to explore thematic overlaps and differences between the two datasets and identify unique insights and innovation gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning with Advanced Open-Source Models\n",
    "- Implement transfer learning by fine-tuning pre-trained open-source models such as RoBERTa, XLNet, Longformer, FLAN-T5, and BART on the text data. Evaluate the model performance using intrinsic measures (e.g., word similarity, clustering quality) before and after fine-tuning. Analyze and quantify the insights gained from the fine-tuned model regarding emerging trends and innovation gaps in cleantech.\n",
    "- Compare the performance of transfer learning with the in-house embeddings. This comparison could be done through evaluating the effectiveness of the embeddings in domain-specific tasks like topic classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2",
   "language": "python",
   "name": "python3122"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
