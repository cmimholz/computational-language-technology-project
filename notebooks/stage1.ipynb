{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkoISsG4ZJim"
   },
   "source": [
    "# Stage 1: Enhanced Data Cleaning, Preprocessing, Exploratory Analysis and Topic Modeling\n",
    "In this notebook, we perform **data cleaning, preprocessing, and exploratory analysis (EDA)** on the Cleantech Media and Google Patent datasets. The goal is to identify **trends, key technologies, and innovation gaps** by analyzing media publications and patents. Furthermore we are going to use some **topic modeling techniques** to discover hidden themes and topics in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cWemGjzyZNzN",
    "outputId": "d0a9aa03-8ebf-410e-87fc-ab9d682418ad"
   },
   "outputs": [],
   "source": [
    "# Mount Google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBt1aOjjZtK7",
    "outputId": "b650f9a4-28b4-4519-db0c-499beb6c6ec2"
   },
   "outputs": [],
   "source": [
    "!pip install contractions\n",
    "!pip install unidecode\n",
    "!pip install num2words\n",
    "!pip install pyspellchecker\n",
    "!pip install langdetect\n",
    "!pip install symspellpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pE8AFXZolBxZ",
    "outputId": "7cd17457-e9a2-4f83-9337-375415a59055"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/wolfgarbe/SymSpell/master/SymSpell/frequency_dictionary_en_82_765.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYE8zGYJZJin"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import contractions\n",
    "import unidecode\n",
    "from tqdm import tqdm\n",
    "from num2words import num2words\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from spellchecker import SpellChecker\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from langdetect import detect\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4s43s-ZvZJio"
   },
   "source": [
    "## Data Collection and Cleaning (Joshua)\n",
    "Before analyzing the data, we first **load, inspect, and clean** the datasets:  \n",
    "\n",
    "- **Load datasets**: We import the **Cleantech Media Dataset** and the **Cleantech Google Patent Dataset** into Pandas DataFrames.  \n",
    "- **Remove duplicates**: Identical or near-identical entries are removed to prevent data bias.  \n",
    "- **Handle missing values**: We check for null or incomplete entries and decide whether to impute, replace, or remove them.  \n",
    "- **Filter relevant information**: Non-informative texts (e.g., generic statements) are removed to ensure high-quality analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weTg9YV4pd0m"
   },
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "data_folder = Path(\"/content/drive/MyDrive/computational-language-technology-project\")\n",
    "media_dataset_path = data_folder / \"data/cleantech_media_dataset_v3_2024-10-28.csv\"\n",
    "google_patent_dataset_path = data_folder / \"data/CleanTech_22-24_updated.json\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQazShVtphZ-"
   },
   "source": [
    "## Cleantech Media Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 886
    },
    "id": "WVNzr2ClZJio",
    "outputId": "636749b5-c737-488d-d79b-425947a8b570"
   },
   "outputs": [],
   "source": [
    "# Load CSV files\n",
    "df_media = pd.read_csv(media_dataset_path, header = 0)\n",
    "\n",
    "print(df_media.info())\n",
    "df_media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TZ-Ym3jwZJip",
    "outputId": "c702b905-bc59-497a-d98b-018233671b40"
   },
   "outputs": [],
   "source": [
    "# Count occurrences of each unique value in the 'domain' column\n",
    "domain_counts = df_media['domain'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(domain_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_YUvlZpctt1"
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe for the processed data\n",
    "df_media_preprocessed = df_media.rename(columns={df_media.columns[0]: 'id'})\n",
    "df_media_preprocessed.drop(columns=['author'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae7BWUThmcDD"
   },
   "source": [
    "TEXT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WsCObtOQnIu6",
    "outputId": "7fd22035-25bd-4ac7-e612-d7bdaf383944"
   },
   "outputs": [],
   "source": [
    "# Extract the 'content' for each ID\n",
    "content_23099 = df_media_preprocessed[df_media_preprocessed['id'] == 23099]['content'].values[0]\n",
    "content_23100 = df_media_preprocessed[df_media_preprocessed['id'] == 23100]['content'].values[0]\n",
    "\n",
    "# Create a comparison table\n",
    "comparison_table = pd.DataFrame({\n",
    "    'ID 23099 Content': [content_23099],\n",
    "    'ID 23100 Content': [content_23100]\n",
    "})\n",
    "\n",
    "# Display the table\n",
    "print(comparison_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "S8Nart9kZJio",
    "outputId": "04df462a-e094-443f-93ff-7efea30499bd"
   },
   "outputs": [],
   "source": [
    "# Convert columns to the required data types\n",
    "df_media_preprocessed['title'] = df_media_preprocessed['title'].astype(str)\n",
    "df_media_preprocessed['content'] = df_media_preprocessed['content'].astype(str)\n",
    "df_media_preprocessed['domain'] = df_media_preprocessed['domain'].astype(str)\n",
    "df_media_preprocessed['url'] = df_media_preprocessed['url'].astype(str)\n",
    "df_media_preprocessed['date'] = pd.to_datetime(df_media_preprocessed['date'], errors='coerce')\n",
    "df_media_preprocessed['id'] = df_media_preprocessed['id'].astype(int)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_ids = df_media_preprocessed[df_media_preprocessed.duplicated(subset=['id'])]\n",
    "print(duplicate_ids)\n",
    "\n",
    "\n",
    "df_media_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkgfNJLfOn8e"
   },
   "outputs": [],
   "source": [
    "# List of unwanted suffixes\n",
    "unwanted_suffixes = [\n",
    "    \"Need help finding the right suppliers?\",\n",
    "    \"Energy XPRT is part of XPRT Media All Rights Reserved. Terms Privacy\",\n",
    "    \"Your email address will not be published. Required fields are marked\",\n",
    "    \"Hi, I 'm Azthena, you can trust me to find commercial scientific answers\",\n",
    "    \"Copyright\",\n",
    "    \"To use the full function of this web site\",\n",
    "    \"EcoFriend.com \",\n",
    "    \"EuropÃ¤ische Vereinigung\",\n",
    "    \"Get updates on the IEA\",\n",
    "    \"About us\",\n",
    "    \"This website uses cookies to\",\n",
    "    \"Recharge is part of DN Media Group\",\n",
    "    \"Solar Industry offers industry participants probing\",\n",
    "    \"Thank you for subscribing to the email newsletter\",\n",
    "    \"This site uses Akismet to reduce spam\"\n",
    "]\n",
    "\n",
    "# Define the prefixes to remove\n",
    "unwanted_prefixes = [\n",
    "    \"By clicking `` Allow All '' you agree \",\n",
    "    \"We use cookies to enhance your experience\",\n",
    "    \"Sign in to get the best natural gas news and data\",\n",
    "    \"Your email address *\",\n",
    "    \"Your password *\",\n",
    "    \"Remember me Continue\",\n",
    "    \"Reset password\",\n",
    "    \"Featured Content\",\n",
    "    \"News & Data Services\",\n",
    "    \"Client Support\",\n",
    "    \"- May 27, 2022 - With the innovation and development\",\n",
    "    \"A consortium of Mitsubishi Power Americas\",\n",
    "    \"Advanced Energy's WaveCapture\",\n",
    "    \"Check out a tour of Solar Turbines ' Configurable Modular\",\n",
    "    \"Benefits of Combined Heat & Power ( Cogeneration) Website\",\n",
    "    \"window.dojoRequire\",\n",
    "    \"Accurate Wind Resource Assessment. Power Performance Verification\",\n",
    "    \"Create a free IEA account to download\",\n",
    "    \"Equip yourself with various operating voltages and advanced control\",\n",
    "    \"FLOWSTAR-Energy is a practical, high resolution model \"\n",
    "    \"Hydrogen Technology Expo & Carbon Capture Technology Expo was held in Bremen\",\n",
    "    \"By Power Vision Engineering Sarl based in Ecublens\",\n",
    "    \"Power Vision Engineering provides various...\",\n",
    "    \"Shenzhen Power Kingdom Co., Ltd. is one of the subsidiaries of Henan Yuguang Gold\",\n",
    "    \"This course investigates\",\n",
    "    \"This course examines\",\n",
    "    \"Welcome to Edinburgh Instruments\",\n",
    "    \"Create a free IEA account to download our reports or subcribe\"\n",
    "]\n",
    "\n",
    "def remove_unwanted_prefixes(content):\n",
    "    if isinstance(content, str):\n",
    "        try:\n",
    "            content_list = eval(content)\n",
    "            if isinstance(content_list, list):\n",
    "                content_list = [\n",
    "                    item for item in content_list\n",
    "                    if not any(item.startswith(prefix) for prefix in unwanted_prefixes)\n",
    "                ]\n",
    "                return str(content_list)  # Convert back to string if needed\n",
    "        except:\n",
    "            pass  # Ignore errors if content is not a valid list\n",
    "    return content\n",
    "\n",
    "# Function to remove everything after any of the unwanted phrases\n",
    "def remove_unwanted_suffixes(content):\n",
    "    if isinstance(content, str):\n",
    "        for phrase in unwanted_suffixes:\n",
    "            index = content.find(phrase)\n",
    "            if index != -1:\n",
    "                return content[:index].strip()  # Keep only the part before the phrase\n",
    "    return content\n",
    "\n",
    "df_media_preprocessed['content'] = df_media_preprocessed['content'].apply(remove_unwanted_suffixes)\n",
    "df_media_preprocessed['content'] = df_media_preprocessed['content'].apply(remove_unwanted_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DzZIACPDbnwY",
    "outputId": "4a231557-903b-4bbf-8100-1b63de964e67"
   },
   "outputs": [],
   "source": [
    "# Extract the 'content' for each ID\n",
    "content_23099 = df_media_preprocessed[df_media_preprocessed['id'] == 23099]['content'].values[0]\n",
    "content_23100 = df_media_preprocessed[df_media_preprocessed['id'] == 23100]['content'].values[0]\n",
    "\n",
    "# Create a comparison table\n",
    "comparison_table = pd.DataFrame({\n",
    "    'ID 23099 Content': [content_23099],\n",
    "    'ID 23100 Content': [content_23100]\n",
    "})\n",
    "\n",
    "# Display the table\n",
    "print(comparison_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7xQgruoDfQP"
   },
   "outputs": [],
   "source": [
    "save_path = \"/content/drive/MyDrive/computational-language-technology-project/cleaned_data/media_dataset_pre-cleaned.csv\"\n",
    "df_media_preprocessed.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBolfnGvtuhH"
   },
   "source": [
    "## Cleantech Google Patent Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UDSxT7z1t0aQ",
    "outputId": "bff733fd-17a6-46ca-ba97-a5171ae2605d"
   },
   "outputs": [],
   "source": [
    "df_google_patents = pd.read_json(google_patent_dataset_path, lines=True)\n",
    "print(df_google_patents.info())\n",
    "df_google_patents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Les28dqCzsj5",
    "outputId": "3d0dcbae-986c-498c-835e-5b6ae19004d9"
   },
   "outputs": [],
   "source": [
    "df_counts = df_google_patents[\"publication_number\"].value_counts().reset_index()\n",
    "df_counts.columns = [\"publication_number\", \"count\"]\n",
    "df_counts = df_counts.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "print(df_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0Ntv5YR2Maq"
   },
   "source": [
    "Differences between those duplicates are in either theirs language or their cpc code. The cpc  (Cooperative Patent Classification) code is part of a classification system used to categorize patents based on their technical content.\n",
    "\n",
    "**Structure of CPC Codes:**\n",
    "\n",
    "CPC codes are hierarchical and consist of:\n",
    "\n",
    "- Section (e.g., Y)\n",
    "- Class (e.g., Y02)\n",
    "- Subclass (e.g., Y02B)\n",
    "- Group (e.g., Y02B10)\n",
    "- Subgroup (e.g., Y02B10/10 or Y02B10/20)\n",
    "\n",
    "A single patent can be classified under multiple CPC codes if its invention covers aspects of different categories.\n",
    "\n",
    "To get rid of all those duplicates, the text in the column \"title\" and \"abstract\" shall be checked if it is in english. If yes, the entry shall be kept and else the duplicate is removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KLgMVU0h4y8_",
    "outputId": "3523bb67-62a9-4791-f3de-850199abcd71"
   },
   "outputs": [],
   "source": [
    "# Function to check if text is in English\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == \"en\"\n",
    "    except:\n",
    "        return False  # Handle cases where text is empty or cannot be detected\n",
    "\n",
    "# Select all publication numbers from df_counts\n",
    "publication_numbers_to_check = df_counts[\"publication_number\"].tolist()\n",
    "\n",
    "# Filter the main dataframe to only include these publication numbers\n",
    "df_filtered = df_google_patents[df_google_patents[\"publication_number\"].isin(publication_numbers_to_check)]\n",
    "\n",
    "# Initialize progress bar for language detection\n",
    "tqdm.pandas(desc=\"Checking language\")\n",
    "\n",
    "# Apply language detection on the filtered dataset\n",
    "df_filtered[\"is_english\"] = df_filtered.progress_apply(\n",
    "    lambda row: is_english(str(row[\"title\"])) and is_english(str(row[\"abstract\"])), axis=1\n",
    ")\n",
    "\n",
    "# Count total and English entries\n",
    "total_count = len(df_filtered)\n",
    "english_count = df_filtered[\"is_english\"].sum()\n",
    "\n",
    "print(f\"Total rows processed: {total_count}\")\n",
    "print(f\"English rows detected: {english_count}\")\n",
    "\n",
    "# Filter only English rows\n",
    "df_english = df_filtered[df_filtered[\"is_english\"]]\n",
    "\n",
    "# Drop duplicates and keep the first English entry per publication_number\n",
    "df_google_patents_preprocessed = df_english.drop_duplicates(subset=\"publication_number\", keep=\"first\")\n",
    "\n",
    "# Check the results\n",
    "df_google_patents_preprocessed.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 929
    },
    "id": "biQsTajeMmKp",
    "outputId": "9bf2744c-a9e9-4d64-814a-89abce4f2211"
   },
   "outputs": [],
   "source": [
    "df_google_patents_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klBE52M5MWdj"
   },
   "outputs": [],
   "source": [
    "save_path = \"/content/drive/MyDrive/computational-language-technology-project/cleaned_data/google_patent_pre-cleaned.csv\"\n",
    "df_google_patents_preprocessed.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iv7hcTrhZJiq"
   },
   "source": [
    "## Text Preprocessing (Joshua)\n",
    "To ensure that the text data is **ready for NLP tasks**, we preprocess it using common natural language processing (NLP) techniques:  \n",
    "\n",
    "- **Tokenization**: Split text into individual words or subwords for better analysis.  \n",
    "- **Stopword Removal**: Common but uninformative words (e.g., \"the\", \"is\", \"and\") are removed.  \n",
    "- **Stemming**: Words are reduced to their root form (e.g., \"developing\" → \"develop\").  \n",
    "- **Lowercasing**: Standardize all text to lowercase to avoid duplicate entries.  \n",
    "\n",
    "These steps improve the quality of text-based analysis and ensure consistency across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o9o-UuzpZJiq",
    "outputId": "e6f2c7ed-2784-4d8c-fc0f-9d57c2da4fbf"
   },
   "outputs": [],
   "source": [
    "PUNCTUATIONS = string.punctuation\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "nltk.download('wordnet')\n",
    "spell = SpellChecker()\n",
    "# Initialize SymSpell\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "\n",
    "# Load dictionary from the local file\n",
    "dictionary_path = \"frequency_dictionary_en_82_765.txt\"\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldf11dggZJir"
   },
   "outputs": [],
   "source": [
    "def remove_emails(text):\n",
    "    return re.sub(r'\\S+@\\S+', '', text) if isinstance(text, str) else text\n",
    "\n",
    "def remove_dates(text):\n",
    "    text = re.sub(r'\\d{1,2}(st|nd|rd|th)?[-./]\\d{1,2}[-./]\\d{2,4}', '', text)\n",
    "    pattern = re.compile(r'(\\d{1,2})?(st|nd|rd|th)?[-./,]?\\s?(of)?\\s?([J|j]an(uary)?|[F|f]eb(ruary)?|[Mm]ar(ch)?|[Aa]pr(il)?|[Mm]ay|[Jj]un(e)?|[Jj]ul(y)?|[Aa]ug(ust)?|[Ss]ep(tember)?|[Oo]ct(ober)?|[Nn]ov(ember)?|[Dd]ec(ember)?)\\s?(\\d{1,2})?(st|nd|rd|th)?\\s?[-./,]?\\s?(\\d{2,4})?')\n",
    "    text = pattern.sub(r'', text)\n",
    "    return text if isinstance(text, str) else text\n",
    "\n",
    "def remove_html(text):\n",
    "    clean_text = BeautifulSoup(text).get_text()\n",
    "    return clean_text\n",
    "\n",
    "def remove_tags_mentions(text):\n",
    "    pattern = re.compile(r'(@\\S+|#\\S+)')\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', PUNCTUATIONS))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word not in STOPWORDS])\n",
    "\n",
    "def remove_whitespaces(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def freq_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    counter = Counter(tokens)\n",
    "    FrequentWords = [word for word, _ in counter.most_common(10)]\n",
    "    return FrequentWords\n",
    "\n",
    "def remove_fw(text):\n",
    "    FrequentWords = freq_words(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    without_fw = [word for word in tokens if word not in FrequentWords]\n",
    "    return ' '.join(without_fw)\n",
    "\n",
    "def rare_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    counter = Counter(tokens)\n",
    "    RareWords = []\n",
    "    number_rare_words = 10\n",
    "    for word, word_count in counter.most_common()[-number_rare_words:]:\n",
    "        RareWords.append(word)\n",
    "    return RareWords\n",
    "\n",
    "def remove_rw(text):\n",
    "    RareWords = rare_words(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    without_rw = [word for word in tokens if word not in RareWords]\n",
    "    return ' '.join(without_rw)\n",
    "\n",
    "def nums_to_words(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        # Check if the word has a number followed by a non-digit (e.g., 45x, 122gw, 122%, etc.)\n",
    "        match = re.match(r\"(-?\\d+)([a-zA-Z%]+)?\", word)\n",
    "\n",
    "        if match:\n",
    "            num_part = match.group(1)  # The number part\n",
    "            suffix = match.group(2)    # The suffix (if any)\n",
    "\n",
    "            try:\n",
    "                num_in_words = num2words(num_part)\n",
    "                if suffix:\n",
    "                    # Ensure the number and suffix are correctly formatted\n",
    "                    new_text.append(f\"{num_in_words} {suffix}\")\n",
    "                else:\n",
    "                    new_text.append(num_in_words)\n",
    "            except Exception as e:\n",
    "                new_text.append(word)  # In case of an error, keep the original word\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def stem_words(text):\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "def accented_to_ascii(text):\n",
    "    return unidecode.unidecode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IousFlorgE-"
   },
   "outputs": [],
   "source": [
    "def correct_spelling(text):\n",
    "    if pd.isna(text):  # Handle NaN values\n",
    "        return text\n",
    "\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "\n",
    "    for word in words:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "        corrected_words.append(suggestions[0].term if suggestions else word)  # Use the best suggestion\n",
    "\n",
    "    return \" \".join(corrected_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bevflxe4ZJiq"
   },
   "source": [
    "### df_media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFQjRtzbfW0W"
   },
   "outputs": [],
   "source": [
    "data_folder = Path(\"/content/drive/MyDrive/computational-language-technology-project\")\n",
    "media_preprocessed_path = data_folder / \"cleaned_data/media_dataset_pre-cleaned.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhooBGNXrrA4"
   },
   "outputs": [],
   "source": [
    "df_media_processed = pd.read_csv(media_preprocessed_path, header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "id": "IU12fleJqPR5",
    "outputId": "cf0406fb-b7d5-4cb4-e566-3bce68b1b2d8"
   },
   "outputs": [],
   "source": [
    "# lower casing\n",
    "df_media_processed['content'] = df_media_processed['content'].apply(lambda x: x.lower())\n",
    "\n",
    "# Call all removals\n",
    "df_media_processed['content'] = df_media_processed['content'].map(remove_emails)\n",
    "df_media_processed['content'] = df_media_processed['content'].map(remove_dates)\n",
    "df_media_processed['content'] = df_media_processed['content'].map(remove_html)\n",
    "df_media_processed['content'] = df_media_processed['content'].map(remove_tags_mentions)\n",
    "df_media_processed['content'] = df_media_processed['content'].map(remove_punctuation)\n",
    "df_media_processed['content'] = df_media_processed['content'].map(remove_stopwords)\n",
    "df_media_processed['content'] = df_media_processed['content'].map(remove_whitespaces)\n",
    "df_media_processed['content'] = df_media_processed['content'].map(remove_fw)\n",
    "df_media_processed['content'] = df_media_processed['content'].map(remove_rw)\n",
    "df_media_processed['content'] = df_media_processed['content'].map(nums_to_words)\n",
    "df_media_processed['content'] = df_media_processed['content'].map(stem_words)\n",
    "df_media_processed['content'] = df_media_processed['content'].map(accented_to_ascii)\n",
    "\n",
    "df_media_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDi4Cg5m0j_k"
   },
   "source": [
    "Correct all spelling errors from the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "id": "M6S-cDjI0ULJ",
    "outputId": "0906cb08-40d2-49b5-c97d-1b76dc95128d"
   },
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"Checking spelling\")\n",
    "df_media_processed.loc[:, 'content'] = df_media_processed.loc[:, 'content'].map(correct_spelling)\n",
    "df_media_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9I-WY-OZJir"
   },
   "source": [
    "To save some time, we save the processed/cleaned dataframe to directly load it for further steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Z330BJsZJir"
   },
   "outputs": [],
   "source": [
    "save_path = \"/content/drive/MyDrive/computational-language-technology-project/cleaned_data/media_dataset_cleaned.csv\"\n",
    "df_media_processed.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVxIC4rZZJir"
   },
   "source": [
    "### df_google_patents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4eySJclck7H"
   },
   "outputs": [],
   "source": [
    "data_folder = Path(\"/content/drive/MyDrive/computational-language-technology-project\")\n",
    "google_patents_preprocessed_path = data_folder / \"cleaned_data/google_patent_pre-cleaned.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mb9Oagh1M3QX"
   },
   "outputs": [],
   "source": [
    "df_google_patents_processed = pd.read_csv(google_patents_preprocessed_path, header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tTIcnOdINBZl",
    "outputId": "838c604d-b8a1-4458-e5c5-6276d575d1e9"
   },
   "outputs": [],
   "source": [
    "# Lower casing\n",
    "df_google_patents_processed[\"abstract\"] = df_google_patents_processed[\"abstract\"].apply(lambda x: x.lower())\n",
    "\n",
    "# Call all removals\n",
    "df_google_patents_processed[\"abstract\"] = df_google_patents_processed[\"abstract\"].map(remove_emails)\n",
    "df_google_patents_processed[\"abstract\"] = df_google_patents_processed[\"abstract\"].map(remove_dates)\n",
    "df_google_patents_processed[\"abstract\"] = df_google_patents_processed[\"abstract\"].map(remove_html)\n",
    "df_google_patents_processed[\"abstract\"] = df_google_patents_processed[\"abstract\"].map(remove_tags_mentions)\n",
    "df_google_patents_processed[\"abstract\"] = df_google_patents_processed[\"abstract\"].map(remove_punctuation)\n",
    "df_google_patents_processed[\"abstract\"] = df_google_patents_processed[\"abstract\"].map(remove_stopwords)\n",
    "df_google_patents_processed[\"abstract\"] = df_google_patents_processed[\"abstract\"].map(remove_whitespaces)\n",
    "df_google_patents_processed[\"abstract\"] = df_google_patents_processed[\"abstract\"].map(remove_fw)\n",
    "df_google_patents_processed[\"abstract\"] = df_google_patents_processed[\"abstract\"].map(remove_rw)\n",
    "df_google_patents_processed[\"abstract\"] = df_google_patents_processed[\"abstract\"].map(nums_to_words)\n",
    "df_google_patents_processed[\"abstract\"] = df_google_patents_processed[\"abstract\"].map(stem_words)\n",
    "df_google_patents_processed[\"abstract\"] = df_google_patents_processed[\"abstract\"].map(accented_to_ascii)\n",
    "\n",
    "df_google_patents_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Y49T0rXC74fA",
    "outputId": "8c05aeef-1df5-4601-d261-cf2626952249"
   },
   "outputs": [],
   "source": [
    "df_google_patents_processed.loc[:, 'abstract'] = df_google_patents_processed.loc[:, 'abstract'].progress_apply(correct_spelling)\n",
    "df_google_patents_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6V9jmXfeom8"
   },
   "outputs": [],
   "source": [
    "save_path = \"/content/drive/MyDrive/computational-language-technology-project/cleaned_data/google_patent_cleaned.csv\"\n",
    "df_google_patents_processed.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwwcfBlHZJis"
   },
   "source": [
    "## Exploratory Data Analysis (Chris)\n",
    "EDA helps us **understand data patterns and distributions** before applying complex NLP models. We perform:  \n",
    "\n",
    "- **Temporal Analysis**: We examine **publication trends** over time to detect emerging Cleantech topics.  \n",
    "- **Named Entity Recognition (NER)**: Identify key **companies, organizations, and technologies** frequently mentioned in the datasets.  \n",
    "- **Word Frequency Analysis**: Find the most common words and phrases across media and patents.  \n",
    "- **Visualization**:  \n",
    "  - **Word Clouds** to showcase frequently occurring terms  \n",
    "  - **Bar Charts** to compare key industry players and technology mentions  \n",
    "  - **Network Graphs** to analyze relationships between companies and technologies  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "media_df = pd.read_csv('../cleaned_data/media_dataset_cleaned.csv')\n",
    "patent_df = pd.read_csv('../cleaned_data/google_patent_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic info\n",
    "print(\"Media Dataset:\")\n",
    "print(media_df.info())\n",
    "print(media_df.head())\n",
    "\n",
    "print(\"\\nPatent Dataset:\")\n",
    "print(patent_df.info())\n",
    "print(patent_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Analysis\n",
    "media_df['date'] = pd.to_datetime(media_df['date'], errors='coerce')\n",
    "patent_df['publication_date'] = patent_df['publication_date'].astype(str).str[:4] + '-' + patent_df['publication_date'].astype(str).str[4:6] + '-' + patent_df['publication_date'].astype(str).str[6:]\n",
    "patent_df['publication_date'] = pd.to_datetime(patent_df['publication_date'], errors='coerce')\n",
    "# Extract month and year for binning\n",
    "media_df['month_year'] = media_df['date'].dt.to_period('M').astype(str)\n",
    "patent_df['month_year'] = patent_df['publication_date'].dt.to_period('M').astype(str)\n",
    "media_df['month'] = media_df['date'].dt.month\n",
    "patent_df['month'] = patent_df['publication_date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution by month.year\n",
    "plt.figure(figsize=(12, 5))\n",
    "media_df['month_year'].value_counts().sort_index().plot(kind='bar', figsize=(15, 5))\n",
    "plt.title(\"Distribution of Articles Over Time (Month-Year)\")\n",
    "plt.xlabel(\"Month-Year\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upper bar chart represents the distribution of media articles over time (grouped by Month-Year). A key observation is the extreme spike in June 2023, where the number of articles is significantly higher than any other month. Other months show a more balanced and steady distribution, with a gradual increase in early 2023 and stabilization afterward.\n",
    "\n",
    "In June 2023, a significant spike in cleantech media coverage was observed, likely due to multiple high-profile industry events. Key events included the Cleantech Forum Asia (Singapore), the ARC Cleantech Innovation Festival (Germany), and the CleanTech Innovation Showcase (Seattle).\n",
    "\n",
    "The concentration of these global events likely contributed to the increased volume of cleantech-related articles in June 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "patent_df['month_year'].value_counts().sort_index().plot(kind='bar', figsize=(15, 5))\n",
    "plt.title(\"Distribution of Patents Over Time (Month-Year)\")\n",
    "plt.xlabel(\"Month-Year\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph illustrates the monthly distribution of patents from January 2022 to September 2024. Patent activity fluctuates, with a notable peaks in August 2022 and June 2023. A significant drop is observed in February 2023. From early 2024 onward, a gradual decline in patent filings is evident, with a sharp drop after April 2024. This decline may indicate incomplete recent data, decreasing patent activity, or shifts in R&D investment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will add some basic text features to the data\n",
    "\n",
    "# Add 'Review lenght'\n",
    "media_df['title_length'] = media_df['title'].astype(str).apply(len)\n",
    "# Add simple token count\n",
    "media_df['number_tokens'] = media_df['title'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Add 'Review lenght'\n",
    "patent_df['abrtract_length'] = patent_df['abstract'].astype(str).apply(len)\n",
    "# Add simple token count\n",
    "patent_df['abstract_tokens'] = patent_df['abstract'].apply(lambda x: len(str(x).split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution suggests that most article titles are relatively short, but some outliers have significantly longer titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that while most patents follow a standardized format, there is significant variation in abstract length and detail, possibly reflecting different technical fields or patent submission styles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_df.groupby('domain').size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table shows the number of articles per media domain. Energy-Xprt (4,181), PV-Magazine (3,093), and Azocleantech (2,488) are the top sources, indicating their strong presence in cleantech reporting. The data follows a long-tail distribution, with a few domains contributing most articles, while niche sources like Biofuels-News (1) and BEX Asia (2) provide minimal coverage. This helps identify key players in the sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_df.groupby('country_code').size().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "patent_df['country_code'].value_counts().sort_index().plot(kind='bar', figsize=(15, 5))\n",
    "plt.title(\"Cleantech-Related Patent Distribution by Country\")\n",
    "plt.xlabel(\"Country\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grouped data shows the number of patents filed per country, sorted in descending order. China (CN) dominates with 26,208 patents, significantly higher than the United States (US) with 881 and WO (World Intellectual Property Organization) with 698. Other notable regions include Europe (EP), South Korea (KR), Australia (AU), and Canada (CA). The distribution highlights China's overwhelming lead in patent filings, suggesting a strong focus on cleantech innovation and intellectual property protection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create boxplot for abstract length by country\n",
    "sns.boxplot(data=patent_df, x='country_code', y='abrtract_length', showfliers=False)\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel(\"Country Code\")\n",
    "plt.ylabel(\"Abstract Length\")\n",
    "plt.title(\"Distribution of Abstract Length by Country\")\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot highlights variations in patent abstract lengths across countries. While some nations have consistently longer or shorter abstracts, it is important to consider the number of patents per country, as nations with fewer patents may show more variability. High-patent countries like CN, US, and EP provide a broader representation, whereas outliers in smaller datasets may not reflect overall trends. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment\n",
    "The following code performs sentiment analysis on text data from two datasets using the VADER SentimentIntensityAnalyzer from the nltk library. The goal is to assess the sentiment polarity of media content and patent abstracts by computing a compound sentiment score for each entry.\n",
    "\n",
    "For each text entry, the polarity score is calculated using VADER’s polarity_scores() method, which returns a dictionary of sentiment scores. The compound score, a single value ranging from -1 (strongly negative) to +1 (strongly positive), is extracted and assigned to a new column 'sentiment'. This score helps to quantitatively assess the sentiment of media content and patent abstracts, enabling further analysis on trends, emotional tone, and public perception. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "media_df['sentiment'] = media_df['content'].fillna('').apply(lambda x: sia.polarity_scores(str(x))['compound'])\n",
    "patent_df['sentiment'] = patent_df['abstract'].fillna('').apply(lambda x: sia.polarity_scores(str(x))['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(media_df['sentiment'], bins=20, kde=True)\n",
    "plt.title(\"Sentiment Distribution in Media Articles\")\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment analysis of media articles shows a strong positive bias, with most sentiment scores clustering around +1.0. Negative and neutral sentiments are significantly less frequent. This suggests that the media coverage in the dataset is predominantly positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(patent_df['sentiment'], bins=20, kde=True)\n",
    "plt.title(\"Sentiment Distribution in Patent Abstracts\")\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment analysis of patent abstracts shows peaks around neutral (0.0) and positive (0.75-1.0) scores. Negative sentiments are less frequent but still present, indicating that patent abstracts tend to be either neutral or slightly optimistic in tone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_df.plot(kind='scatter', x='title_length', y='sentiment', figsize=(15,7))\n",
    "plt.xlabel('Title Length')\n",
    "plt.ylabel('Sentiment')\n",
    "plt.title('Scatter Plot of Title Length vs Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most titles are short (<100 characters) and strongly positive (1.0 sentiment). Longer titles are rare and show mixed sentiment. No clear correlation between title length and sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_df.plot(kind='scatter', x='abrtract_length', y='sentiment', figsize=(15,7))\n",
    "plt.xlabel('Abstract Length')\n",
    "plt.ylabel('Sentiment')\n",
    "plt.title('Scatter Plot of Abstract Length vs Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most abstracts are under 500 characters and show a wide range of sentiment values. Sentiment distribution appears balanced, with both positive and negative scores. Longer abstracts are rare and also as before do not show a clear sentiment trend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_bigram(media_df['title'], 20)\n",
    "bigrams = pd.DataFrame(common_words, columns = ['word' , 'count'])\n",
    "\n",
    "bigrams.plot(kind='barh', x='word', color='cadetblue', width=0.5, figsize=(5,6))\n",
    "plt.xlabel('count')\n",
    "plt.ylabel('word')\n",
    "plt.title('Media Bigrams w/o stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common bigrams highlight key topics like solar energy, energy storage, and renewable energy. \"PV magazine\" appears most frequently, suggesting a major industry source. These terms indicate the primary focus areas in cleantech media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_trigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_trigram(media_df['title'], 20)\n",
    "trigrams = pd.DataFrame(common_words, columns = ['word' , 'count'])\n",
    "\n",
    "trigrams.plot(kind='barh', x='word', color='cadetblue', width=0.5, figsize=(5,6))\n",
    "plt.xlabel('count')\n",
    "plt.ylabel('word')\n",
    "plt.title('Media Trigrams w/o stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"PV magazine international\" appears most frequently, reinforcing its role as a dominant industry source. The most frequent trigrams emphasize solar energy, energy storage, and energy management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_bigram(patent_df['abstract'].dropna(), 20)\n",
    "bigrams = pd.DataFrame(common_words, columns = ['word' , 'count'])\n",
    "\n",
    "bigrams.plot(kind='barh', x='word', color='cadetblue', width=0.5, figsize=(5,6))\n",
    "plt.xlabel('count')\n",
    "plt.ylabel('word')\n",
    "plt.title('Patent Bigrams w/o stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent bigrams highlight technical terminology such as \"technic field,\" \"invent disclose,\" and \"solar energy.\" The frequent use of \"model\" and \"relay\" suggests a strong focus on engineering and innovation in patent documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_trigram(patent_df['abstract'].dropna(), 20)\n",
    "trigrams = pd.DataFrame(common_words, columns = ['word' , 'count'])\n",
    "\n",
    "trigrams.plot(kind='barh', x='word', color='cadetblue', width=0.5, figsize=(5,6))\n",
    "plt.xlabel('count')\n",
    "plt.ylabel('word')\n",
    "plt.title('Patent Trigrams w/o stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trigrams further refine key concepts, with phrases like \"relay technic field,\" \"belong technic field,\" and \"comprise follow step.\" These terms indicate a focus on technical processes, energy management, and patent disclosures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER) using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition (NER) using spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ in ['ORG', 'PRODUCT']]\n",
    "    \n",
    "\n",
    "media_df['entities'] = media_df['content'].fillna('').apply(lambda x: extract_entities(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../cleaned_data/media_dataset_cleaned_entity.csv\"\n",
    "media_df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_df['entities'] = patent_df['abstract'].fillna('').apply(lambda x: extract_entities(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../cleaned_data/google_patent_cleaned_entity.csv\"\n",
    "patent_df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-occurrence Matrix\n",
    "entity_pairs = []\n",
    "for entities in media_df['entities'].dropna():\n",
    "    entity_pairs.extend([(e1, e2) for i, e1 in enumerate(entities) for e2 in entities[i+1:]])\n",
    "for entities in patent_df['entities'].dropna():\n",
    "    entity_pairs.extend([(e1, e2) for i, e1 in enumerate(entities) for e2 in entities[i+1:]])\n",
    "\n",
    "co_occurrence = Counter(entity_pairs)\n",
    "co_occurrence_df = pd.DataFrame(co_occurrence.items(), columns=['Pair', 'Count']).sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Visualizing Co-occurrence Matrix\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(y=co_occurrence_df['Pair'].astype(str)[:20], x=co_occurrence_df['Count'][:20], palette=\"viridis\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Entity Pair\")\n",
    "plt.title(\"Top 20 Entity Co-occurrences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Visualization\n",
    "G = nx.Graph()\n",
    "for (e1, e2), count in co_occurrence.items():\n",
    "    G.add_edge(e1, e2, weight=count)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "pos = nx.spring_layout(G, k=0.5)\n",
    "nx.draw(G, pos, with_labels=True, node_size=50, font_size=10)\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels={(e1, e2): count for (e1, e2), count in co_occurrence.items() if count > 5}, font_size=8)\n",
    "plt.title(\"Entity Co-occurrence Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Clustering on Entity Co-occurrences\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "entity_texts = [' '.join(pair) for pair in co_occurrence_df['Pair']]\n",
    "X = vectorizer.fit_transform(entity_texts)\n",
    "\n",
    "num_clusters = 10\n",
    "kmeans = KMeans(n_clusters=num_clusters, max_iter=50, tol=0.01, random_state=42)\n",
    "kmeans.fit(X)\n",
    "clusters = kmeans.labels_.tolist()\n",
    "\n",
    "# Display Clustered Entities\n",
    "centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = np.array(vectorizer.get_feature_names_out())\n",
    "for i, cluster_terms in enumerate(centroids):\n",
    "    print(\"Cluster #{}: {}\".format(i, \", \".join(terms[cluster_terms[:8]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Word Clouds\n",
    " Word Cloud Analysis\n",
    "To visualize the most frequently used words in media articles and patent abstracts, we generate word clouds. This technique helps identify dominant themes and key terms by displaying words in varying sizes based on their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Clouds\n",
    "media_text = ' '.join(media_df['content'].dropna().astype(str))\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(media_text)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud for Media Articles\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dominant terms include \"two thousand,\" \"projects,\", \"product,\" \"industry\" and \"power\". This suggests that the media coverage focuses heavily on industrial developments and energy projects related to cleantech.\n",
    "\n",
    "The presence of numerical values like \"one thousand,\" \"twenty,\" and \"thirty\" indicates frequent mentions of years, financial figures, or project scales. Words such as \"develop,\" \"supply,\" \"generate,\" and \"design\" emphasize a strong focus on innovation, infrastructure, and renewable energy initiatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_text = ' '.join(patent_df['abstract'].dropna().astype(str))\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(patent_text)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud for Patent Abstracts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word cloud for patent abstracts highlights key terms frequently appearing in the dataset. Dominant words such as \"technic,\" \"field,\" \"disclose,\" \"invent,\" and \"provide\" suggest a strong focus on technical innovation, disclosures, and inventions.\n",
    "\n",
    "Terms like \"solar,\" \"energy,\" \"relay,\" and \"gene\" indicate that many patents relate to renewable energy, electrical systems, and biotechnology. Additionally, words such as \"structure,\" \"process,\" and \"assembly\" emphasize the technical and engineering aspects of patent filings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zxQ6PDOZJis"
   },
   "source": [
    "## Topic Modeling (Blerta)\n",
    "To **identify hidden themes and emerging trends**, we apply topic modeling techniques on both datasets:  \n",
    "\n",
    "- **Latent Dirichlet Allocation (LDA)** and **Non-Negative Matrix Factorization (NMF)** to uncover broad thematic structures.  \n",
    "- **Top2Vec** and **BERTopic** for **more dynamic and context-aware topic modeling**.  \n",
    "- **Comparing Media vs. Patents**:  \n",
    "  - Which Cleantech topics are **gaining media attention** but **not patented** yet?  \n",
    "  - Are **patents aligned with market trends**, or do they focus on different areas?  \n",
    "\n",
    "By the end of this step, we will have a **structured view of the Cleantech landscape**, highlighting **key trends, players, and technological opportunities**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for topic modeling\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from top2vec import Top2Vec\n",
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patent Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to apply the above mentioned techniques first for the patent dataset, and afterwards for the media dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_number</th>\n",
       "      <th>application_number</th>\n",
       "      <th>country_code</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>inventor</th>\n",
       "      <th>cpc_code</th>\n",
       "      <th>is_english</th>\n",
       "      <th>month_year</th>\n",
       "      <th>month</th>\n",
       "      <th>abrtract_length</th>\n",
       "      <th>abstract_tokens</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CN-117151396-A</td>\n",
       "      <td>CN-202311109834-A</td>\n",
       "      <td>CN</td>\n",
       "      <td>Distributed economic scheduling method for win...</td>\n",
       "      <td>disclose method solar ethan firstly solar prov...</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>['HU PENGFEI', 'LI ZIMENG']</td>\n",
       "      <td>G06Q50/06</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>12</td>\n",
       "      <td>371</td>\n",
       "      <td>53</td>\n",
       "      <td>-0.5267</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN-117147382-A</td>\n",
       "      <td>CN-202310985511-A</td>\n",
       "      <td>CN</td>\n",
       "      <td>Device for monitoring hydrogen atom crossing g...</td>\n",
       "      <td>invent provide atom use spam relay technic fie...</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>['MA ZHAOXIANG', 'WANG CHENGXU', 'LIU ZHONGLI']</td>\n",
       "      <td>G01N13/00</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>12</td>\n",
       "      <td>539</td>\n",
       "      <td>75</td>\n",
       "      <td>-0.3182</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CN-113344288-B</td>\n",
       "      <td>CN-202110717505-A</td>\n",
       "      <td>CN</td>\n",
       "      <td>Cascade hydropower station group water level p...</td>\n",
       "      <td>disclose device compute readable storage mediu...</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>[]</td>\n",
       "      <td>G06Q10/04</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>12</td>\n",
       "      <td>330</td>\n",
       "      <td>46</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CN-117153944-A</td>\n",
       "      <td>CN-202311209193-A</td>\n",
       "      <td>CN</td>\n",
       "      <td>Heterojunction solar cell, preparation method ...</td>\n",
       "      <td>apply provide hetero solar cell prepare method...</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>['TONG HONGBO', 'JIN YUPENG']</td>\n",
       "      <td>H01L31/074</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>12</td>\n",
       "      <td>328</td>\n",
       "      <td>44</td>\n",
       "      <td>0.8834</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CN-116911695-B</td>\n",
       "      <td>CN-202311167289-A</td>\n",
       "      <td>CN</td>\n",
       "      <td>Flexible resource adequacy evaluation method a...</td>\n",
       "      <td>invent relay method device electro system belo...</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>[]</td>\n",
       "      <td>H02J2203/20</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-12</td>\n",
       "      <td>12</td>\n",
       "      <td>487</td>\n",
       "      <td>68</td>\n",
       "      <td>-0.8779</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  publication_number application_number country_code  \\\n",
       "0     CN-117151396-A  CN-202311109834-A           CN   \n",
       "1     CN-117147382-A  CN-202310985511-A           CN   \n",
       "2     CN-113344288-B  CN-202110717505-A           CN   \n",
       "3     CN-117153944-A  CN-202311209193-A           CN   \n",
       "4     CN-116911695-B  CN-202311167289-A           CN   \n",
       "\n",
       "                                               title  \\\n",
       "0  Distributed economic scheduling method for win...   \n",
       "1  Device for monitoring hydrogen atom crossing g...   \n",
       "2  Cascade hydropower station group water level p...   \n",
       "3  Heterojunction solar cell, preparation method ...   \n",
       "4  Flexible resource adequacy evaluation method a...   \n",
       "\n",
       "                                            abstract publication_date  \\\n",
       "0  disclose method solar ethan firstly solar prov...       2023-12-01   \n",
       "1  invent provide atom use spam relay technic fie...       2023-12-01   \n",
       "2  disclose device compute readable storage mediu...       2023-12-01   \n",
       "3  apply provide hetero solar cell prepare method...       2023-12-01   \n",
       "4  invent relay method device electro system belo...       2023-12-01   \n",
       "\n",
       "                                          inventor     cpc_code  is_english  \\\n",
       "0                      ['HU PENGFEI', 'LI ZIMENG']    G06Q50/06        True   \n",
       "1  ['MA ZHAOXIANG', 'WANG CHENGXU', 'LIU ZHONGLI']    G01N13/00        True   \n",
       "2                                               []    G06Q10/04        True   \n",
       "3                    ['TONG HONGBO', 'JIN YUPENG']   H01L31/074        True   \n",
       "4                                               []  H02J2203/20        True   \n",
       "\n",
       "  month_year  month  abrtract_length  abstract_tokens  sentiment entities  \n",
       "0    2023-12     12              371               53    -0.5267       []  \n",
       "1    2023-12     12              539               75    -0.3182       []  \n",
       "2    2023-12     12              330               46     0.0000       []  \n",
       "3    2023-12     12              328               44     0.8834       []  \n",
       "4    2023-12     12              487               68    -0.8779       []  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lets again have a general look at the dataset to see what we are working with\n",
    "# load patent dataset directly from the .csv file \n",
    "patent_modeling = pd.read_csv(\"../cleaned_data/google_patent_cleaned_entity.csv\")  \n",
    "\n",
    "patent_modeling.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preparation\n",
    "abstract = patent_modeling[\"abstract\"].fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model for the patent dataset:\n",
      "   Topic  Word 1   Word 2  Word 3  Word 4  Word 5  Word 6       Word 7   Word 8    Word 9  Word 10\n",
      " Topic 1 connect  arrange     end  second  common provide     comprise assembly    plural     body\n",
      " Topic 2    step   method   optic  obtain calculi  accord       follow    value determine     base\n",
      " Topic 3    heat exchange    pump    tank  circus    pipe      collect condemns     water   vapour\n",
      " Topic 4 prepare    mater thereof improve   apply  method photovoltaic   reduce   product    field\n",
      " Topic 5 electro  battery storage  supply convert     use        panel    solar    charge    store\n",
      " Topic 6  rotate    drive   shaft   motor  adjust  meghan         gear     angl       rod   driven\n",
      " Topic 7    gene    power    wind turbine   blade  invent       driven      set   product comprise\n",
      " Topic 8 control   detect  sensor monitor  signal automat       switch    regal    device intellig\n",
      " Topic 9  instal  include   model   mount     set  energy        panel   adjust       fix    solar\n",
      "Topic 10   plate      rod   block  groove support    wall          fix    inner       end  fixedly\n"
     ]
    }
   ],
   "source": [
    "# step 1: TF-IDF transformation\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words=\"english\")\n",
    "tfidf_matrix = vectorizer.fit_transform(abstract)\n",
    "\n",
    "# step 2: NMF model training\n",
    "nmf = NMF(n_components=10, random_state=42, l1_ratio=0.5)\n",
    "W = nmf.fit_transform(tfidf_matrix)  \n",
    "H = nmf.components_  \n",
    "\n",
    "# step 3: show top words for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topics.append([f\"Topic {topic_idx+1}\"] + top_words)\n",
    "    \n",
    "    df_topics = pd.DataFrame(topics, columns=[\"Topic\"] + [f\"Word {i+1}\" for i in range(n_top_words)])\n",
    "    print(df_topics.to_string(index=False))\n",
    "\n",
    "print(\"\\nTopics in NMF model for the patent dataset:\")\n",
    "print_top_words(nmf, feature_names, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model for the patent dataset:\n",
      "   Topic     Word 1   Word 2  Word 3    Word 4   Word 5       Word 6   Word 7     Word 8   Word 9 Word 10\n",
      " Topic 1     method   accord    step     value  acquire       obtain  control  determine  calculi  follow\n",
      " Topic 2    connect      end   plate       rod  arrange       rotate      fix      block  fixedly support\n",
      " Topic 3 hydropower  station problem     engin   method    construct conserve     invent     sold   water\n",
      " Topic 4       heat exchange    pump      gene     tank       circus   energy        use  collect  invent\n",
      " Topic 5    electro  control  supply   battery  connect      convert     gene        use   charge storage\n",
      " Topic 6      optic     step  method establish consider      calculi   obtain constraint     base  invent\n",
      " Topic 7    arrange comprise provide   surface     form photovoltaic  connect  structure   energy   model\n",
      " Topic 8     rotate     gene   drive     blade  arrange        shaft  connect     energy comprise support\n",
      " Topic 9    prepare    mater thereof    method    apply        field   obtain   electron   energy improve\n",
      "Topic 10    connect    model  instal   include    panel          fix   rotate     energy    mount     set\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Convert the text data (abstract) to a TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words=\"english\")\n",
    "tfidf_matrix = vectorizer.fit_transform(abstract)\n",
    "\n",
    "# Step 2: Fit the LDA model\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "# Step 3: print top words\n",
    "def print_top_words_lda(model, feature_names, n_top_words):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topics.append([f\"Topic {topic_idx+1}\"] + top_words)\n",
    "    \n",
    "    # Creating a DataFrame to display the results in a nice tabular format\n",
    "    df_topics = pd.DataFrame(topics, columns=[\"Topic\"] + [f\"Word {i+1}\" for i in range(n_top_words)])\n",
    "    print(df_topics.to_string(index=False))\n",
    "\n",
    "# Example usage after fitting the LDA model\n",
    "print(\"\\nTopics in LDA model for the patent dataset:\")\n",
    "print_top_words_lda(lda, vectorizer.get_feature_names_out(), 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 17:20:28,841 - top2vec - INFO - Pre-processing documents for training\n",
      "2025-03-15 17:20:33,063 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
      "2025-03-15 17:20:35,450 - top2vec - INFO - Creating joint document/word embedding\n",
      "2025-03-15 17:27:04,402 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2025-03-15 17:27:12,274 - top2vec - INFO - Finding dense areas of documents\n",
      "2025-03-15 17:27:15,455 - top2vec - INFO - Finding topics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "    relay (score: 0.4614)\n",
      "    photovoltaic (score: 0.4317)\n",
      "    implement (score: 0.4258)\n",
      "    electroplate (score: 0.4151)\n",
      "    highvoltag (score: 0.3942)\n",
      "    panel (score: 0.3890)\n",
      "    machinery (score: 0.3851)\n",
      "    design (score: 0.3836)\n",
      "    energyconserv (score: 0.3825)\n",
      "    turbine (score: 0.3823)\n",
      "    circuit (score: 0.3801)\n",
      "    energyconcerv (score: 0.3785)\n",
      "    technology (score: 0.3733)\n",
      "    lowvoltag (score: 0.3698)\n",
      "    appliance (score: 0.3643)\n",
      "    motor (score: 0.3633)\n",
      "    conduit (score: 0.3623)\n",
      "    manual (score: 0.3529)\n",
      "    system (score: 0.3516)\n",
      "    form (score: 0.3509)\n",
      "    feature (score: 0.3507)\n",
      "    procedure (score: 0.3501)\n",
      "    heater (score: 0.3468)\n",
      "    field (score: 0.3463)\n",
      "    facility (score: 0.3442)\n",
      "    heatconduct (score: 0.3433)\n",
      "    hydroelectric (score: 0.3391)\n",
      "    project (score: 0.3389)\n",
      "    instruct (score: 0.3379)\n",
      "    construct (score: 0.3379)\n",
      "    inlet (score: 0.3379)\n",
      "    achieve (score: 0.3375)\n",
      "    solar (score: 0.3347)\n",
      "    process (score: 0.3343)\n",
      "    outlet (score: 0.3334)\n",
      "    reactor (score: 0.3312)\n",
      "    architecture (score: 0.3306)\n",
      "    chassis (score: 0.3263)\n",
      "    pvt (score: 0.3244)\n",
      "    subsystem (score: 0.3232)\n",
      "    conveyor (score: 0.3231)\n",
      "    invent (score: 0.3208)\n",
      "    electrify (score: 0.3205)\n",
      "    apparatus (score: 0.3199)\n",
      "    method (score: 0.3192)\n",
      "    electromagnet (score: 0.3190)\n",
      "    energysav (score: 0.3175)\n",
      "    electro (score: 0.3160)\n",
      "    fixture (score: 0.3159)\n",
      "    thermalarrest (score: 0.3157)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the 'abstract' column as a list of text documents\n",
    "documents_patent = patent_modeling[\"abstract\"].dropna().tolist()  # Drop NaN values\n",
    "\n",
    "# Train Top2Vec model\n",
    "model_patent = Top2Vec(documents_patent, speed=\"learn\", workers=4)  # Adjusted speed to learn for medium quality\n",
    "num_topics = model_patent.get_num_topics()\n",
    "\n",
    "topics_words, word_scores, topic_nums = model_patent.get_topics()\n",
    "\n",
    "for i in range(num_topics):\n",
    "    words = topics_words[i]\n",
    "    scores = word_scores[i]\n",
    "    topic_num = topic_nums[i]\n",
    "    \n",
    "    print(f\"Topic {topic_num}\")\n",
    "    for word, score in zip(words, scores):\n",
    "        print(f\"    {word} (score: {score:.4f})\")\n",
    "    print(\"\\n\")\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores represent the relevance or importance of that word to the given topic. These scores are often based on word embeddings and semantic similarity within the topic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The five words that appear in all three topic modeling results are:\n",
    "\n",
    "- photovoltaic\n",
    "- panel\n",
    "- electro\n",
    "- invent\n",
    "- method\n",
    "\n",
    "These words are consistently identified as significant across the different topic modeling approaches, indicating they are central themes in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Media dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the same now for the media dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>month_year</th>\n",
       "      <th>month</th>\n",
       "      <th>title_length</th>\n",
       "      <th>number_tokens</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93320</td>\n",
       "      <td>XPeng Delivered ~100,000 Vehicles In 2021</td>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>chines startup shown drama auto product campus...</td>\n",
       "      <td>cleantechnica</td>\n",
       "      <td>https://cleantechnica.com/2022/01/02/xpeng-del...</td>\n",
       "      <td>2022-01</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "      <td>0.9906</td>\n",
       "      <td>['transit corp', 'ada']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93321</td>\n",
       "      <td>Green Hydrogen: Drop In Bucket Or Big Splash?</td>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>laid plan build largest product fail world int...</td>\n",
       "      <td>cleantechnica</td>\n",
       "      <td>https://cleantechnica.com/2022/01/02/its-a-gre...</td>\n",
       "      <td>2022-01</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "      <td>0.9081</td>\n",
       "      <td>['china alliance', 'mitsubishi example', 'toyo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98159</td>\n",
       "      <td>World’ s largest floating PV plant goes online...</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>intern switch array china a shandong deploy tw...</td>\n",
       "      <td>pv-magazine</td>\n",
       "      <td>https://www.pv-magazine.com/2022/01/03/worlds-...</td>\n",
       "      <td>2022-01</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>14</td>\n",
       "      <td>0.9198</td>\n",
       "      <td>['suzhou china', 'kwh electro']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98158</td>\n",
       "      <td>Iran wants to deploy 10 GW of renewables over ...</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>iranian author current eight go project submit...</td>\n",
       "      <td>pv-magazine</td>\n",
       "      <td>https://www.pv-magazine.com/2022/01/03/iran-wa...</td>\n",
       "      <td>2022-01</td>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.0057</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31128</td>\n",
       "      <td>Eastern Interconnection Power Grid Said ‘ Bein...</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>daily dpi infrastructure i news access electro...</td>\n",
       "      <td>naturalgasintel</td>\n",
       "      <td>https://www.naturalgasintel.com/eastern-interc...</td>\n",
       "      <td>2022-01</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>0.9814</td>\n",
       "      <td>['naturalgasintelcom']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title        date  \\\n",
       "0  93320          XPeng Delivered ~100,000 Vehicles In 2021  2022-01-02   \n",
       "1  93321      Green Hydrogen: Drop In Bucket Or Big Splash?  2022-01-02   \n",
       "2  98159  World’ s largest floating PV plant goes online...  2022-01-03   \n",
       "3  98158  Iran wants to deploy 10 GW of renewables over ...  2022-01-03   \n",
       "4  31128  Eastern Interconnection Power Grid Said ‘ Bein...  2022-01-03   \n",
       "\n",
       "                                             content           domain  \\\n",
       "0  chines startup shown drama auto product campus...    cleantechnica   \n",
       "1  laid plan build largest product fail world int...    cleantechnica   \n",
       "2  intern switch array china a shandong deploy tw...      pv-magazine   \n",
       "3  iranian author current eight go project submit...      pv-magazine   \n",
       "4  daily dpi infrastructure i news access electro...  naturalgasintel   \n",
       "\n",
       "                                                 url month_year  month  \\\n",
       "0  https://cleantechnica.com/2022/01/02/xpeng-del...    2022-01      1   \n",
       "1  https://cleantechnica.com/2022/01/02/its-a-gre...    2022-01      1   \n",
       "2  https://www.pv-magazine.com/2022/01/03/worlds-...    2022-01      1   \n",
       "3  https://www.pv-magazine.com/2022/01/03/iran-wa...    2022-01      1   \n",
       "4  https://www.naturalgasintel.com/eastern-interc...    2022-01      1   \n",
       "\n",
       "   title_length  number_tokens  sentiment  \\\n",
       "0            41              6     0.9906   \n",
       "1            45              8     0.9081   \n",
       "2            83             14     0.9198   \n",
       "3            93             17    -0.0057   \n",
       "4            71             11     0.9814   \n",
       "\n",
       "                                            entities  \n",
       "0                            ['transit corp', 'ada']  \n",
       "1  ['china alliance', 'mitsubishi example', 'toyo...  \n",
       "2                    ['suzhou china', 'kwh electro']  \n",
       "3                                                 []  \n",
       "4                             ['naturalgasintelcom']  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load cleaned dataset\n",
    "media_modeling = pd.read_csv(\"../cleaned_data/media_dataset_cleaned_entity.csv\")  \n",
    "\n",
    "media_modeling.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model for the model dataset:\n",
      "   Topic       Word 1      Word 2    Word 3     Word 4        Word 5    Word 6      Word 7    Word 8      Word 9  Word 10\n",
      " Topic 1       sector    industry   support technology       sustain    invest      govern    policy       renew thousand\n",
      " Topic 2        women     plastic   inspire     planet         spoke     organ         day interview environment   nation\n",
      " Topic 3       instal      design      slut    battery       electro    invert     provide       kwh       inter  control\n",
      " Topic 4     thousand   thousands    thirty      seven          thre     price     seventy  increase        year      nin\n",
      " Topic 5      content       pleas       res  copyright       protect      want      cooper        pm        cell     said\n",
      " Topic 6      vehicle         eve       car      tesla cleantechnica      time        like    charge        make    thing\n",
      " Topic 7      product manufacture  industry      engin    technology      cell      design   process        heat    mater\n",
      " Topic 8 azocleantech       kelli interview      infra          talk     oxide      vapour transform     combust   robust\n",
      " Topic 9      project     develop     renew  construct          said agreement        plan   company   portfolio    local\n",
      "Topic 10     research         use     study   universe     scientist    potent temperature    differ     process    water\n"
     ]
    }
   ],
   "source": [
    "# Same for the media dataset\n",
    "## preparation\n",
    "content = media_modeling[\"content\"].fillna(\"\")\n",
    "\n",
    "# step 1: TF-IDF transformation\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words=\"english\")\n",
    "tfidf_matrix = vectorizer.fit_transform(content)\n",
    "\n",
    "# step 2: NMF model training\n",
    "nmf = NMF(n_components=10, random_state=42, l1_ratio=0.5, max_iter=1000)\n",
    "W = nmf.fit_transform(tfidf_matrix)  \n",
    "H = nmf.components_\n",
    "\n",
    "# step 3: show top words for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topics.append([f\"Topic {topic_idx+1}\"] + top_words)\n",
    "    \n",
    "    df_topics = pd.DataFrame(topics, columns=[\"Topic\"] + [f\"Word {i+1}\" for i in range(n_top_words)])\n",
    "    print(df_topics.to_string(index=False))\n",
    "\n",
    "print(\"\\nTopics in NMF model for the model dataset:\")\n",
    "print_top_words(nmf, feature_names, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had to increase the maximum iterations from the default of 200 to 1000 because we received a warning that the iteration limit had been reached. Increasing it helped improve convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model for the media dataset:\n",
      "   Topic       Word 1    Word 2   Word 3      Word 4        Word 5    Word 6     Word 7   Word 8      Word 9 Word 10\n",
      " Topic 1      product    design      use manufacture         engin   provide technology industry        heat    slut\n",
      " Topic 2     thousand   develop  project       renew         state    invest   industry  support        plan  govern\n",
      " Topic 3        drill      heat  develop    thousand       project   explore   resource   potent      energy    deep\n",
      " Topic 4          use     model   design        cell       product    instal      panel      kwh  technology      pm\n",
      " Topic 5        women   plastic  inspire      planet         spoke interview      organ      day environment  nation\n",
      " Topic 6      vehicle  thousand      eve         car cleantechnica      time      tesla  electro        like     use\n",
      " Topic 7      content     pleas      res   copyright      thousand      want     cooper  protect        said develop\n",
      " Topic 8     thousand   project     year       renew        thirty      said    develop    price       japan  invest\n",
      " Topic 9 azocleantech interview research         use   environment     study      kelli    oxide      nature  effect\n",
      "Topic 10   technology  industry  provide        slut       develop   project   thousand   custom     company   offer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Convert the text data (abstract) to a TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words=\"english\")\n",
    "tfidf_matrix = vectorizer.fit_transform(media_modeling[\"content\"].fillna(\"\"))\n",
    "\n",
    "# Step 2: Fit the LDA model\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "# Step 3: print top words\n",
    "def print_top_words_lda(model, feature_names, n_top_words):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topics.append([f\"Topic {topic_idx+1}\"] + top_words)\n",
    "    \n",
    "    # Creating a DataFrame to display the results in a nice tabular format\n",
    "    df_topics = pd.DataFrame(topics, columns=[\"Topic\"] + [f\"Word {i+1}\" for i in range(n_top_words)])\n",
    "    print(df_topics.to_string(index=False))\n",
    "\n",
    "# Example usage after fitting the LDA model\n",
    "print(\"\\nTopics in LDA model for the media dataset:\")\n",
    "print_top_words_lda(lda, vectorizer.get_feature_names_out(), 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 17:29:47,571 - top2vec - INFO - Pre-processing documents for training\n",
      "2025-03-15 17:30:07,904 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
      "2025-03-15 17:30:09,792 - top2vec - INFO - Creating joint document/word embedding\n",
      "2025-03-15 17:43:36,564 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2025-03-15 17:43:40,725 - top2vec - INFO - Finding dense areas of documents\n",
      "2025-03-15 17:43:43,264 - top2vec - INFO - Finding topics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "    renewable (score: 0.3471)\n",
      "    energyeffici (score: 0.3302)\n",
      "    demand (score: 0.3257)\n",
      "    shortage (score: 0.3230)\n",
      "    infrastructure (score: 0.3218)\n",
      "    sector (score: 0.3054)\n",
      "    conserve (score: 0.3007)\n",
      "    industrialscal (score: 0.2990)\n",
      "    corp (score: 0.2986)\n",
      "    pipeline (score: 0.2949)\n",
      "    influx (score: 0.2949)\n",
      "    consortium (score: 0.2929)\n",
      "    utilityscal (score: 0.2837)\n",
      "    exxonmobil (score: 0.2812)\n",
      "    costeffici (score: 0.2796)\n",
      "    reliance (score: 0.2786)\n",
      "    forklift (score: 0.2766)\n",
      "    photovoltaic (score: 0.2722)\n",
      "    survey (score: 0.2716)\n",
      "    powertrain (score: 0.2690)\n",
      "    stakeholder (score: 0.2688)\n",
      "    maintain (score: 0.2685)\n",
      "    downstream (score: 0.2676)\n",
      "    industrywid (score: 0.2661)\n",
      "    biofuel (score: 0.2658)\n",
      "    energysav (score: 0.2652)\n",
      "    industrials (score: 0.2650)\n",
      "    pv (score: 0.2640)\n",
      "    highvoltag (score: 0.2633)\n",
      "    overhaul (score: 0.2627)\n",
      "    highlevel (score: 0.2570)\n",
      "    tesla (score: 0.2568)\n",
      "    electrify (score: 0.2562)\n",
      "    ecostruxur (score: 0.2557)\n",
      "    quarterly (score: 0.2544)\n",
      "    envelop (score: 0.2544)\n",
      "    diversify (score: 0.2543)\n",
      "    electrotechn (score: 0.2530)\n",
      "    enterprise (score: 0.2525)\n",
      "    cleantech (score: 0.2517)\n",
      "    industrial (score: 0.2511)\n",
      "    industrylead (score: 0.2506)\n",
      "    leverage (score: 0.2496)\n",
      "    energyhub (score: 0.2495)\n",
      "    multilevel (score: 0.2475)\n",
      "    turbocharge (score: 0.2469)\n",
      "    comprise (score: 0.2465)\n",
      "    statelevel (score: 0.2462)\n",
      "    crew (score: 0.2450)\n",
      "    greenfield (score: 0.2444)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the 'abstract' column as a list of text documents\n",
    "documents_patent = media_modeling[\"content\"].dropna().tolist()  # Drop NaN values\n",
    "\n",
    "# Train Top2Vec model\n",
    "model_patent = Top2Vec(documents_patent, speed=\"learn\", workers=4)  # Adjusted speed to learn for medium quality\n",
    "num_topics = model_patent.get_num_topics()\n",
    "\n",
    "topics_words, word_scores, topic_nums = model_patent.get_topics()\n",
    "\n",
    "for i in range(num_topics):\n",
    "    words = topics_words[i]\n",
    "    scores = word_scores[i]\n",
    "    topic_num = topic_nums[i]\n",
    "    \n",
    "    print(f\"Topic {topic_num}\")\n",
    "    for word, score in zip(words, scores):\n",
    "        print(f\"    {word} (score: {score:.4f})\")\n",
    "    print(\"\\n\")\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four words that appear in all three topic modeling results are:\n",
    "\n",
    "- industry\n",
    "- tesla\n",
    "- renew (appears as renew or renewable)\n",
    "- electro\n",
    "\n",
    "Since these words consistently appear across different topic modeling approaches, they likely represent key themes in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion topic modelling:\n",
    "\n",
    "When looking at the words which appear in all three topic models of the media dataset, we see that there is one word which also appears in the patent dataset beeing: *electro*. The other words have gained media attention but are not pattented yet. Furthermore the patended top words are aligned with market trends with those beeing photovoltaic, panel and electro beeing the top words which were found in all three topic modeling methods. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkGqdkAeZJis"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.12.2",
   "language": "python",
   "name": "python3122"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
